package exporter

import (
	"encoding/json"
	"fmt"
	"log"
	"strings"

	"github.com/databrickslabs/terraform-provider-databricks/common"
	"github.com/databrickslabs/terraform-provider-databricks/compute"
	"github.com/databrickslabs/terraform-provider-databricks/identity"
	"github.com/databrickslabs/terraform-provider-databricks/storage"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
)

func (ic *importContext) importCluster(c *compute.Cluster) error {
	if c == nil {
		return nil
	}
	for _, is := range c.InitScripts {
		if is.Dbfs != nil {
			ic.Emit(&resource{
				Resource: "databricks_dbfs_file",
				ID:       is.Dbfs.Destination,
			})
		}
	}
	if c.AwsAttributes != nil {
		ic.Emit(&resource{
			Resource: "databricks_instance_profile",
			ID:       c.AwsAttributes.InstanceProfileArn,
		})
	}
	if c.InstancePoolID != "" {
		ic.Emit(&resource{
			Resource: "databricks_instance_pool",
			ID:       c.InstancePoolID,
		})
	}
	if c.PolicyID != "" {
		ic.Emit(&resource{
			Resource: "databricks_cluster_policy",
			ID:       c.PolicyID,
		})
	}
	return nil
}

func (ic *importContext) importLibraries(d *schema.ResourceData, s map[string]*schema.Schema) error {
	var cll compute.ClusterLibraryList
	err := common.DataToStructPointer(d, s, &cll)
	if err != nil {
		return err
	}
	for _, lib := range cll.Libraries {
		if strings.HasPrefix(lib.Whl, "dbfs:") {
			ic.Emit(&resource{
				Resource: "databricks_dbfs_file",
				ID:       lib.Whl,
			})
		}
		if strings.HasPrefix(lib.Jar, "dbfs:") {
			ic.Emit(&resource{
				Resource: "databricks_dbfs_file",
				ID:       lib.Jar,
			})
		}
		if strings.HasPrefix(lib.Egg, "dbfs:") {
			ic.Emit(&resource{
				Resource: "databricks_dbfs_file",
				ID:       lib.Egg,
			})
		}
	}
	return nil
}

func (ic *importContext) cacheGroups() error {
	if len(ic.allGroups) == 0 {
		log.Printf("[INFO] Caching groups in memory ...")
		groupsAPI := identity.NewGroupsAPI(ic.Context, ic.Client)
		g, err := groupsAPI.Filter("")
		if err != nil {
			return err
		}
		ic.allGroups = g.Resources
		log.Printf("[INFO] Cached %d groups", len(ic.allGroups))
	}
	return nil
}

// func (ic *importContext) cacheUsers() error {
// 	if len(ic.allUsers) == 0 {
// 		// workspace has at least one user, always.
// 		log.Printf("[INFO] Fetching users into in-memory cache")
// 		usersAPI := identity.NewUsersAPI(ic.Client)
// 		users, err := usersAPI.Filter("active eq true")
// 		if err != nil {
// 			return err
// 		}
// 		ic.allUsers = users
// 		log.Printf("[INFO] Cached %d users", len(users))
// 	}
// 	return nil
// }

func (ic *importContext) findUserByName(name string) (u identity.ScimUser, err error) {
	a := identity.NewUsersAPI(ic.Context, ic.Client)
	users, err := a.Filter(fmt.Sprintf("userName eq '%s'", name))
	if err != nil {
		return
	}
	if len(users) == 0 {
		err = fmt.Errorf("User %s not found", name)
		return
	}
	u = users[0]
	return
	// if err := ic.cacheUsers(); err != nil {
	// 	return
	// }
	// for _, _u := range ic.allUsers {
	// 	if _u.UserName == name {
	// 		u = _u
	// 		return
	// 	}
	// }
	// err = fmt.Errorf("User %s not found", name)
	// return
}

func (ic *importContext) emitIfDbfsFile(path string) {
	if strings.HasPrefix(path, "dbfs:") {
		ic.Emit(&resource{
			Resource: "databricks_dbfs_file",
			ID:       path,
		})
	}
}

func (ic *importContext) refreshMounts() error {
	if ic.mountMap != nil {
		return nil
	}
	commandAPI := ic.Client.CommandExecutor(ic.Context)
	clustersAPI := compute.NewClustersAPI(ic.Context, ic.Client)
	cluster, err := clustersAPI.GetOrCreateRunningCluster("terraform-mount")
	if err != nil {
		return err
	}
	log.Printf("[INFO] Refreshing worskpace-wide mounts")
	mountMap, err := ic.getMountsThroughCluster(commandAPI, cluster.ClusterID)
	if err != nil {
		return err
	}
	log.Printf("[INFO] Found %d worskpace-wide mounts", len(mountMap))
	ic.mountMap = map[string]mount{}
	for k, v := range mountMap {
		ic.mountMap[k] = mount{
			URL: v,
			// cluster id is needed for AWS S3 mounts, that may
			// be visible for every cluster
			ClusterID: cluster.ClusterID,
		}
	}
	if !ic.Client.IsAzure() {
		profiles, err := identity.NewInstanceProfilesAPI(ic.Context, ic.Client).List()
		if err != nil {
			return err
		}
		for _, instanceProfile := range profiles {
			log.Printf("[INFO] Refreshing mounts accessible by %s", instanceProfile.InstanceProfileArn)
			profileCluster, err := storage.GetOrCreateMountingClusterWithInstanceProfile(
				clustersAPI, instanceProfile.InstanceProfileArn)
			if err != nil {
				return err
			}
			profileMountMap, err := ic.getMountsThroughCluster(commandAPI, profileCluster.ClusterID)
			if err != nil {
				return err
			}
			i := 0
			for k, v := range profileMountMap {
				if _, has := ic.mountMap[k]; has {
					continue
				}
				i++
				ic.mountMap[k] = mount{
					URL:             v,
					InstanceProfile: instanceProfile.InstanceProfileArn,
				}
			}
			if i > 0 {
				log.Printf("[INFO] Found %d mounts accessible by %s",
					len(profileMountMap), instanceProfile.InstanceProfileArn)
			}
		}
	}
	return nil
}

var getReadableMountsCommand = `
import scala.concurrent._
import scala.concurrent.duration._
import ExecutionContext.Implicits.global
import scala.concurrent.{Await, Future}
import com.fasterxml.jackson.databind.{DeserializationFeature, ObjectMapper}
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule

val readableMounts = dbutils.fs.mounts.par.map { mount =>
try {
	Await.result(Future {
		dbutils.fs.ls(mount.mountPoint)
		(mount.mountPoint
			.replace("/mnt/", "")
			.stripSuffix("/"), 
		 mount.source)
	}, 5.second)
} catch {
	case _ : Throwable => (null, mount.source)
}
}.seq.filter {
	mount => mount._1 != null
} toMap

val mapper = new ObjectMapper() with ScalaObjectMapper
mapper.registerModule(DefaultScalaModule)
mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)

println(mapper.writeValueAsString(readableMounts))`

func (ic *importContext) getMountsThroughCluster(
	commandAPI common.CommandExecutor, clusterID string) (mm map[string]string, err error) {
	// Scala has actually working timeout handling, compared to Python
	j, err := commandAPI.Execute(clusterID, "scala", getReadableMountsCommand)
	if err != nil {
		return
	}
	lines := strings.Split(j, "\n")
	err = json.Unmarshal([]byte(lines[0]), &mm)
	return
}
