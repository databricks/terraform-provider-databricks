[
{
	"uri": "/",
	"title": "Databricks Terraform Provider",
	"tags": [],
	"description": "",
	"content": "Lets lay some bricks! Provider Download After downloading and untaring/unzipping the artifact make sure you move it to ~/.terraform.d/plugins/:\nmkdir -p ~/.terraform.d/plugins/ \u0026amp;\u0026amp; cp terraform-provider-databricks ~/.terraform.d/plugins/terraform-provider-databricks Feedback  Please provide feedback!  "
},
{
	"uri": "/quickstart/",
	"title": "Databricks Terraform Provider",
	"tags": [],
	"description": "",
	"content": "Under Construction "
},
{
	"uri": "/quickstart/overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "Quickstart: Building and Using the Provider Setup Please note that there is a Makefile which contains all the commands you would need to run this project.\nThis code base to contribute to requires the following software:\n golang 1.13.X terraform v0.12.x make command  To make sure everything is installed correctly please run the following commands:\nTesting go installation:\n$ go version go version go1.13.3 darwin/amd64 Testing terraform installation:\n$ terraform --version Terraform v0.12.19 Your version of Terraform is out of date! The latest version is 0.12.24. You can update by downloading from https://www.terraform.io/downloads.html Testing make installation:\n$ make --version GNU Make 3.81 Copyright (C) 2006 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. This program built for i386-apple-darwin11.3.0 Downloading the source code and installing the artifact  After installing golang, terraform, and make you will now build the artifact.  $ go get -v -u github.com/databrickslabs/databricks-terraform \u0026amp;\u0026amp; cd $GOPATH/src/github.com/databrickslabs/databricks-terraform ⚠ If you are fetching from a private repository please use the following command:\n$ GOSUMDB=off GOPROXY=direct go get -v -u github.com/databrickslabs/databricks-terraform \u0026amp;\u0026amp; cd $GOPATH/src/github.com/databrickslabs/databricks-terraform  When you are in the root directory of the repository please run:  $ make build   Locate your terraform plugins directory or the root folder of your terraform code\n  Copy the terraform-provider-databricks artifact to that terraform plugins locations\n  $ mkdir -p ~/.terraform.d/plugins/ \u0026amp;\u0026amp; cp terraform-provider-databricks ~/.terraform.d/plugins/terraform-provider-databricks Now your plugin for the Databricks Terraform provider is installed correctly. You can actually use the provider.\nBasic Terraform example Sample terraform code\nprovider \u0026#34;databricks\u0026#34; { host = \u0026#34;http://databrickshost.com\u0026#34; token = \u0026#34;dapitokenhere\u0026#34; } resource \u0026#34;databricks_scim_user\u0026#34; \u0026#34;my-user\u0026#34; { user_name = join(\u0026#34;\u0026#34;, [\u0026#34;test-user\u0026#34;, \u0026#34;+\u0026#34;,count.index,\u0026#34;@databricks.com\u0026#34;]) display_name = \u0026#34;Test User\u0026#34; } Then run terraform init then terraform apply to apply the hcl code to your databricks workspace.\nPlease refer to the detailed documentation provided in the html documentation for detailed use of the providers.\nAlso refer to these examples for more scenarios.\nProvider Documentation Provider documentation can be located in the releases tab and documentation is packaged up along with the binary of choice.\nDocker commands To install and build the code if you dont want to install golang, terraform, etc. All you need is docker and git.\nFirst make sure you clone the repository and you are in the directory.\nThen build the docker image with this command:\n$ docker build -t databricks-terraform . Then run the execute the terraform binary via the following command and volume mount. Make sure that you are in the directory with the terraform code. The following command you can execute the following commands and additional ones as part of the terraform binary.\n$ docker run -it -v $(pwd):/workpace -w /workpace databricks-terraform init $ docker run -it -v $(pwd):/workpace -w /workpace databricks-terraform plan $ docker run -it -v $(pwd):/workpace -w /workpace databricks-terraform apply Project Components Databricks Terraform Provider Resources State    Resource Implemented Import Support Acceptance Tests Documentation Reviewed Finalize Schema     databricks_token ✅ ⬜ ✅ ✅ ⬜ ⬜   databricks_secret_scope ✅ ⬜ ✅ ✅ ⬜ ⬜   databricks_secret ✅ ⬜ ✅ ✅ ⬜ ⬜   databricks_secret_acl ✅ ⬜ ✅ ✅ ⬜ ⬜   databricks_instance_pool ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_scim_user ✅ ⬜ ✅ ✅ ⬜ ⬜   databricks_scim_group ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_notebook ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_cluster ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_job ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_dbfs_file ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_dbfs_file_sync ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_instance_profile ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_aws_s3_mount ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_azure_blob_mount ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_azure_adls_gen1_mount ✅ ⬜ ⬜ ✅ ⬜ ⬜   databricks_azure_adls_gen2_mount ✅ ⬜ ⬜ ✅ ⬜ ⬜    Databricks Terraform Data Sources State    Data Source Implemented Acceptance Tests Documentation Reviewed     databricks_notebook ✅ ⬜ ⬜ ⬜   databricks_notebook_paths ✅ ⬜ ⬜ ⬜   databricks_dbfs_file ✅ ⬜ ⬜ ⬜   databricks_dbfs_file_paths ✅ ⬜ ⬜ ⬜   databricks_zones ⬜ ⬜ ⬜ ⬜   databricks_runtimes ⬜ ⬜ ⬜ ⬜   databricks_instance_pool ⬜ ⬜ ⬜ ⬜   databricks_scim_user ⬜ ⬜ ⬜ ⬜   databricks_scim_group ⬜ ⬜ ⬜ ⬜   databricks_cluster ⬜ ⬜ ⬜ ⬜   databricks_job ⬜ ⬜ ⬜ ⬜   databricks_mount ⬜ ⬜ ⬜ ⬜   databricks_instance_profile ⬜ ⬜ ⬜ ⬜   databricks_database ⬜ ⬜ ⬜ ⬜   databricks_table ⬜ ⬜ ⬜ ⬜    Testing ⬜ Integration tests should be run at a client level against both azure and aws to maintain sdk parity against both apis (currently only on one cloud)\n⬜ Terraform acceptance tests should be run against both aws and azure to maintain parity of provider between both cloud services (currently only on one cloud)\nProject Support Please note that all projects in the /databrickslabs github account are provided for your exploration only, and are not formally supported by Databricks with Service Level Agreements (SLAs). They are provided AS-IS and we do not make any guarantees of any kind. Please do not submit a support ticket relating to any issues arising from the use of these projects.\nAny issues discovered through the use of this project should be filed as GitHub Issues on the Repo. They will be reviewed as time permits, but there are no formal SLAs for support.\n"
},
{
	"uri": "/quickstart/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "A Terraform provider for Databricks workspace components\nInstalling databricks-terraform with Go  Install Go 1.13. For previous versions, you may have to set your $GOPATH manually, if you haven\u0026rsquo;t done it yet visit here. Install Terraform 0.12.x from here and save it into /usr/local/bin/terraform folder (create it if it doesn\u0026rsquo;t exists). This provider DOES NOT SUPPORT Terraform 0.12 or above. Download the code by issuing a go get command.  # Download the source code for databricks-terraform # and build the needed binary, by saving it inside $GOPATH/bin $ go get -u github.com/databrickslabs/databricks-terraform # After fetching the code base we will switch into the directory for the code base. $ cd $GOPATH/src/github.com/databrickslabs/databricks-terraform # Once in the directory you will run the build using the make command provided by the make file $ make build # Once the file is made we will then move the file to where terraform can pick it up $ mkdir -p ~/.terraform.d/plugins/ \u0026amp;\u0026amp; cp terraform-provider-db ~/.terraform.d/plugins/terraform-provider-db If you wish to uninstall the binary simply remove the file from the directory.\n$ rm /usr/local/bin/terraform-provider-db Using databricks-terraform with Docker (TBD!) "
},
{
	"uri": "/provider/",
	"title": "Provider",
	"tags": [],
	"description": "",
	"content": "Databricks Provider The Databricks provider is what is used to interact with the Databricks resources. This needs to be configured so that terraform can provision resources in your Databricks workspace on your behalf.\nExample Usages Token Basic Auth Azure SP Auth  provider \u0026#34;databricks\u0026#34; { host = \u0026#34;http://databricks.domain.com\u0026#34; token = \u0026#34;dapitokenhere\u0026#34; } resource \u0026#34;databricks_scim_user\u0026#34; \u0026#34;my-user\u0026#34; { user_name = \u0026#34;test-user@databricks.com\u0026#34; display_name = \u0026#34;Test User\u0026#34; }   provider \u0026#34;databricks\u0026#34; { host = \u0026#34;http://databricks.domain.com\u0026#34; token = base64encode(\u0026#34;${var.user}:${var.password}\u0026#34;) auth_type = \u0026#34;BASIC\u0026#34; } resource \u0026#34;databricks_scim_user\u0026#34; \u0026#34;my-user\u0026#34; { user_name = \u0026#34;test-user@databricks.com\u0026#34; display_name = \u0026#34;Test User\u0026#34; }   provider \u0026#34;azurerm\u0026#34; { client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id subscription_id = var.subscription_id } resource \u0026#34;azurerm_databricks_workspace\u0026#34; \u0026#34;demo_test_workspace\u0026#34; { location = \u0026#34;centralus\u0026#34; name = \u0026#34;my-workspace-name\u0026#34; resource_group_name = var.resource_group managed_resource_group_name = var.managed_resource_group_name sku = \u0026#34;premium\u0026#34; } provider \u0026#34;databricks\u0026#34; { azure_auth = { managed_resource_group = azurerm_databricks_workspace.demo_test_workspace.managed_resource_group_name azure_region = azurerm_databricks_workspace.demo_test_workspace.location workspace_name = azurerm_databricks_workspace.demo_test_workspace.name resource_group = azurerm_databricks_workspace.demo_test_workspace.resource_group_name client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id subscription_id = var.subscription_id } } resource \u0026#34;databricks_scim_user\u0026#34; \u0026#34;my-user\u0026#34; { user_name = \u0026#34;test-user@databricks.com\u0026#34; display_name = \u0026#34;Test User\u0026#34; }     Please be aware that hard coding credentials is not something that is recommended. It may be best if you store the credentials environment variables or use tfvars file.\n Authentication There are currently two supported methods to authenticate into the Databricks platform to create resources.\n API Token Azure Service Principal Authentication  Azure Service Principal Authentication will only work on Azure Databricks where as the API Token authentication will work on both Azure and AWS\n API Token Databricks hostname for the workspace and api token can be provided here. This configuration is very similar to the Databricks CLI\nprovider \u0026#34;databricks\u0026#34; { host = \u0026#34;http://databricks.domain.com\u0026#34; token = \u0026#34;dapitokenhere\u0026#34; }  Please be aware that hard coding credentials is not something that is recommended. It may be best if you store the credentials environment variables or use tfvars file.\n Azure Service Principal Auth provider \u0026#34;databricks\u0026#34; { azure_auth = { managed_resource_group = \u0026#34;${azurerm_databricks_workspace.sri_test_workspace.managed_resource_group_name}\u0026#34; azure_region = \u0026#34;${azurerm_databricks_workspace.sri_test_workspace.location}\u0026#34; workspace_name = \u0026#34;${azurerm_databricks_workspace.sri_test_workspace.name}\u0026#34; resource_group = \u0026#34;${azurerm_databricks_workspace.sri_test_workspace.resource_group_name}\u0026#34; client_id = \u0026#34;${var.client_id}\u0026#34; client_secret = \u0026#34;${var.client_secret}\u0026#34; tenant_id = \u0026#34;${var.tenant_id}\u0026#34; subscription_id = \u0026#34;${var.subscription_id}\u0026#34; } } Environment variables The following variables can be passed via environment variables:\n host → HOST token → TOKEN subscription_id → ARM_SUBSCRIPTION_ID client_secret → ARM_CLIENT_SECRET client_id → ARM_CLIENT_ID tenant_id → ARM_TENANT_ID  For example you can have the following provider definition:\nprovider \u0026#34;databricks\u0026#34; {} Then run the following code and the following environment variables will be injected into the provider.\n$ export HOST=\u0026#34;http://databricks.domain.com\u0026#34; $ export TOKEN=\u0026#34;dapitokenhere\u0026#34; $ terraform plan Provider Argument Reference The following arguments are supported by the db provider block:\nhost  This is the host of the Databricks workspace. This is will be a url that you use to login to your workspace. Alternatively you can provide this value as an environment variable HOST.\n token:  This is the api token to authenticate into the workspace. Alternatively you can provide this value as an environment variable TOKEN.\n azure-auth:  Usage azure_auth = { azure_region = \u0026#34;centralus\u0026#34; managed_resource_group = \u0026#34;my-databricks-managed-rg\u0026#34; workspace_name = \u0026#34;test-managed-workspace\u0026#34; resource_group = \u0026#34;1-test-rg\u0026#34; client_id = var.client_id client_secret = var.client_secret tenant_id = var.tenant_id subscription_id = var.subscription_id }    This is the authentication required to authenticate to the Databricks via an azure service principal that has access to the workspace. This is optional as you can use the api token based auth. The azure_auth block contains the following arguments:     managed_resource_group - This is the managed workgroup id when the Databricks workspace is provisioned\n  azure_region - This is the azure region in which your workspace is deployed.\n  workspace_name - This is the name of your Azure Databricks Workspace.\n  resource_group - This is the resource group in which your Azure Databricks Workspace resides in.\n  subscription_id - This is the Azure Subscription id in which your Azure Databricks Workspace resides in. Alternatively you can provide this value as an environment variable ARM_SUBSCRIPTION_ID.\n  client_secret - This is the Azure Enterprise Application (Service principal) client secret. This service principal requires contributor access to your Azure Databricks deployment. Alternatively you can provide this value as an environment variable ARM_CLIENT_SECRET.\n  client_id - This is the Azure Enterprise Application (Service principal) client id. This service principal requires contributor access to your Azure Databricks deployment. Alternatively you can provide this value as an environment variable ARM_CLIENT_ID.\n  tenant_id - This is the Azure Active Directory Tenant id in which the Enterprise Application (Service Principal) resides in. Alternatively you can provide this value as an environment variable ARM_TENANT_ID.\n     "
},
{
	"uri": "/resources/aws_s3_mount/",
	"title": "aws_s3_mount",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_aws_s3_mount This resource given a cluster id will help you create, get and delete a aws s3 mount.\nIt is important to understand that this will start up the cluster if the cluster is terminated. The read and refresh terraform command will require a cluster and make take some time to validate mount.\n You can locate the mount in dbfs at dbfs:/mnt/\u0026lt;mount_name\u0026gt;\n Currently only supports the use of IAM roles and not S3 keys and secrets.\n Example Usage resource \u0026#34;databricks_aws_s3_mount\u0026#34; \u0026#34;my_custom_mount4\u0026#34; { cluster_id = \u0026#34;####-######-hello###\u0026#34; s3_bucket_name = \u0026#34;my-s3-bucket-123\u0026#34; mount_name = \u0026#34;my_s3_bucket_mount\u0026#34; } Argument Reference The following arguments are supported:\n- cluster_id:  (Required) This is the cluster id in which the mount will be initalized from. If the cluster is in a terminated state it will be started.\n - s3_bucket_name:  (Required) This is the S3 bucket that you are trying to mount.\n - mount_name:  (Required) This is the name of the mount that will represent where the data will be landed.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  Identifier for the mount.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/azure_adls_gen1_mount/",
	"title": "azure_adls_gen1_mount",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_azure_adls_gen1_mount This resource given a cluster id will help you create, get and delete a azure data lake gen 1(ADLS gen 1) mount using a service principal/enterprise ad application which will provide you a client id and client secret to authenticate.\nIt is important to understand that this will start up the cluster if the cluster is terminated. The read and refresh terraform command will require a cluster and make take some time to validate mount.\n You can locate the mount in dbfs at dbfs:/mnt/\u0026lt;mount_name\u0026gt;\n Example Usage resource \u0026#34;databricks_azure_adls_gen1_mount\u0026#34; \u0026#34;my_custom_mount3\u0026#34; { cluster_id = \u0026#34;####-######-pear###\u0026#34; storage_resource_name = \u0026#34;my_storage_resource_name\u0026#34; mount_name = \u0026#34;my_adls_gen1_mount\u0026#34; tenant_id = \u0026#34;????????-????-????-????-????????????\u0026#34; client_id = \u0026#34;????????-????-????-????-????????????\u0026#34; client_secret_scope = \u0026#34;my_adls_client_secret_scope\u0026#34; client_secret_key= \u0026#34;my_adls_client_secret_key\u0026#34; } Argument Reference The following arguments are supported:\n- cluster_id:  (Required) This is the cluster id in which the mount will be initalized from. If the cluster is in a terminated state it will be started.\n - storage_resource_name:  (Required) The name of the storage resource in which the data is for ADLS gen 1. This is what you are trying to mount.\n - spark_conf_prefix:  (Required) This is the spark configuration prefix for adls gen 1 mount. The options are fs.adl, dfs.adls. Use fs.adl for runtime 6.0 and above for the clusters. Otherwise use dfs.adls. The default value is: fs.adl.\n - mount_directory:  (Optional) This is optional if you want to add an additional directory that you wish to mount. This must start with a \u0026ldquo;/\u0026rdquo;\n - mount_name:  (Required) The name of the folder that you want to mount to in dbfs. You can access the data from /mnt/\u0026lt;mount_name\u0026gt;\n - tenant_id:  (Required) This is your azure directory tenant id. This is required for creating the mount.\n - client_id:  (Required) This is the client_id for the enterprise application for the service principal.\n - token_secret_scope:  (Required) This is the secret scope in which your service principal/enterprise app client secret will be stored.\n - token_secret_key:  (Required) This is the secret key in which your service principal/enterprise app client secret will be stored.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  Identifier for a adls gen 1 mount.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/azure_adls_gen2_mount/",
	"title": "azure_adls_gen2_mount",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_azure_adls_gen2_mount This resource given a cluster id will help you create, get and delete a azure data lake gen 2 (ADLS gen 2) mount using a service principal/enterprise ad application which will provide you a client id and client secret to authenticate.\nIt is important to understand that this will start up the cluster if the cluster is terminated. The read and refresh terraform command will require a cluster and make take some time to validate mount.\n You can locate the mount in dbfs at dbfs:/mnt/\u0026lt;mount_name\u0026gt;\n Example Usage resource \u0026#34;databricks_azure_adls_gen2_mount\u0026#34; \u0026#34;my_custom_mount2\u0026#34; { cluster_id = \u0026#34;####-######-pear###\u0026#34; container_name = \u0026#34;my_storage_container\u0026#34; storage_account_name = \u0026#34;mystorageaccountname\u0026#34; mount_name = \u0026#34;my_cool_adls_gen2_mount\u0026#34; tenant_id = \u0026#34;????????-????-????-????-????????????\u0026#34; client_id = \u0026#34;????????-????-????-????-????????????\u0026#34; client_secret_scope = \u0026#34;my_adls_client_secret_scope\u0026#34; client_secret_key= \u0026#34;my_adls_client_secret_key\u0026#34; } Argument Reference The following arguments are supported:\n- cluster_id:  (Required) This is the cluster id in which the mount will be initalized from. If the cluster is in a terminated state it will be started.\n - container_name:  (Required) The container in which the data is. This is what you are trying to mount.\n - storage_account_name:  (Required) The name of the storage account in which the data is. This is what you are trying to mount.\n - directory:  (Optional) This is optional if you want to add an additional directory that you wish to mount. This must start with a \u0026ldquo;/\u0026rdquo;\n - mount_name:  (Required) The name of the folder that you want to mount to in dbfs. You can access the data from /mnt/\u0026lt;mount_name\u0026gt;\n - tenant_id:  (Required) This is your azure directory tenant id. This is required for creating the mount.\n - client_id:  (Required) This is the client_id for the enterprise application for the service principal.\n - secret_scope:  (Required) This is the secret scope in which your service principal/enterprise app client secret will be stored.\n - secret_key:  (Required) This is the secret key in which your service principal/enterprise app client secret will be stored.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  Identifier for a adls gen 2 mount.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/azure_blob_mount/",
	"title": "azure_blob_mount",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_azure_blob_mount This resource given a cluster id will help you create, get and delete a azure blob storage mount using SAS token or storage account access keys.\nIt is important to understand that this will start up the cluster if the cluster is terminated. The read and refresh terraform command will require a cluster and make take some time to validate mount.\n You can locate the mount in dbfs at dbfs:/mnt/\u0026lt;mount_name\u0026gt;\n Example Usage With the below resource the data gets mounted to /mnt/my_cool_dbfs_mount\nresource \u0026#34;databricks_azure_blob_mount\u0026#34; \u0026#34;my_custom_mount\u0026#34; { cluster_id = \u0026#34;####-######-pear###\u0026#34; container_name = \u0026#34;my_storage_container\u0026#34; directory = \u0026#34;/my_custom_folder\u0026#34; storage_account_name = \u0026#34;mystorageaccountname\u0026#34; mount_name = \u0026#34;my_cool_blob_storage_mount\u0026#34; auth_type = \u0026#34;SAS\u0026#34; token_secret_scope = \u0026#34;my_secret_scope\u0026#34; token_secret_key = \u0026#34;my_secret_key\u0026#34; } Argument Reference The following arguments are supported:\n- cluster_id:  (Required) This is the cluster id in which the mount will be initalized from. If the cluster is in a terminated state it will be started.\n - container_name:  (Required) The container in which the data is. This is what you are trying to mount.\n - storage_account_name:  (Required) The name of the storage account in which the data is. This is what you are trying to mount.\n - directory:  (Optional) This is optional if you want to add an additional directory that you wish to mount. This must start with a \u0026ldquo;/\u0026rdquo;\n - mount_name:  (Required) The name of the folder that you want to mount to in dbfs. You can access the data from /mnt/\u0026lt;mount_name\u0026gt;\n - auth_type:  (Required) This is the auth type for blob storage. This can either be SAS tokens or account access keys.\n - token_secret_scope:  (Required) This is the secret scope in which your auth type token exists in.\n - token_secret_key:  (Required) This is the secret key in which your auth type token exists in.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id of the azure blob storage mount.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/cluster/",
	"title": "cluster",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_cluster This resource allows you to create, update, and delete clusters.\nExample Usage resource \u0026#34;databricks_cluster\u0026#34; \u0026#34;my-cluster\u0026#34; { num_workers = \u0026#34;2\u0026#34; cluster_name = \u0026#34;sri-test\u0026#34; spark_version = \u0026#34;6.4.x-scala2.11\u0026#34; node_type_id = \u0026#34;i3.2xlarge\u0026#34; autotermination_minutes = 30 aws_attributes { availability = \u0026#34;ON_DEMAND\u0026#34; zone_id = \u0026#34;us-east-1\u0026#34; instance_profile_arn = \u0026#34;arn:aws:iam::999999999999:instance-profile/s3-access\u0026#34; } } Argument Reference The following arguments are supported:\n- num_workers (Optional) :  (Optional) Number of worker nodes that this cluster should have. A cluster has one Spark Driver and num_workers Executors for a total of num_workers + 1 Spark node\n - autoscale (Optional) :    Parameters needed in order to automatically scale clusters up and down based on load.   Usage resource \u0026#34;databricks_cluster\u0026#34; \u0026#34;my-cluster\u0026#34; { ... autoscale { min_workers = 2 max_workers = 3 } ... }   min_workers - (Optional) The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.\n  max_workers - (Optional) The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.\n     - cluster_name:  (Optional) Cluster name. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n - spark_version:  (Optional) The Spark version of the cluster. A list of available Spark versions can be retrieved by using the Runtime Versions API call. This field is required.\n - spark_conf:  (Optional) An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively.\n - aws_attributes (Optional) :    Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used.   Usage resource \u0026#34;databricks_cluster\u0026#34; \u0026#34;my-cluster\u0026#34; { ... aws_attributes { zone_id = \u0026#34;us-east-1\u0026#34; availability = \u0026#34;SPOT\u0026#34; spot_bid_price_percent = 100 instance_profile_arn = \u0026#34;arn:aws:iam::999999999999:instance-profile/custom-s3-access-instance-profile\u0026#34; first_on_demand = 1 ebs_volume_type = \u0026#34;GENERAL_PURPOSE_SSD\u0026#34; ebs_volume_count = 1 ebs_volume_size = 32 } ... }   zone_id - (Required) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like “us-west-2a”. The provided availability zone must be in the same region as the Databricks deployment. For example, “us-west-2a” is not a valid zone ID if the Databricks deployment resides in the “us-east-1” region.\n  availability - (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Note: If first_on_demand is zero, this availability type will be used for the entire cluster.\n  spot_bid_price_percent - (Optional) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000.\n  instance_profile_arn - (Optional) Nodes for this cluster will only be placed on AWS instances with this instance profile. If omitted, nodes will be placed on instances without an instance profile. The instance profile must have previously been added to the Databricks environment by an account administrator.\n  first_on_demand - (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.\n  ebs_volume_type - (Optional) The type of EBS volumes that will be launched with this cluster. GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD\n  ebs_volume_count - (Optional) The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.\n  ebs_volume_size - (Optional) The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).\n     - driver_node_type_id:  (Optional) The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above.\n - node_type_id:  (Optional) This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List Node Types API call. This field is required.\n - ssh_public_keys:  (Optional) SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. Up to 10 keys can be specified.\n - custom_tags:  (Optional) Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS instances and EBS volumes) with these tags in addition to default_tags.\n - cluster_log_conf (Optional) :  Usage DBFS S3  cluster_log_conf { dbfs { destination = \u0026#34;dbfs:/my/path/in/dbfs\u0026#34; } }   cluster_log_conf { s3 { destination = \u0026#34;s3:/my/path/in/dbfs\u0026#34; region = \u0026#34;us-east-1\u0026#34; endpoint = \u0026#34;https://s3-us-east-1.amazonaws.com\u0026#34; enable_encryption = true encryption_type = \u0026#34;sse-kms\u0026#34; kms_key = \u0026#34;my-kms-key-here\u0026#34; canned_acl = \u0026#34;bucket-owner-full-control\u0026#34; } }       The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is \u0026lt;destination\u0026gt;/\u0026lt;cluster-id\u0026gt;/driver, while the destination of executor logs is \u0026lt;destination\u0026gt;/\u0026lt;cluster-id\u0026gt;/executor.     dbfs - Configuration for the dbfs cluster logs configuration\n destination - (Optional) DBFS location of cluster log. destination must be provided. For example, \u0026ldquo;dbfs:/home/cluster_log\u0026rdquo;    s3 - Configuration for the s3 cluster logs configuration\n  destination - (Optional) S3 destination, e.g. s3://my-bucket/some-prefix You must configure the cluster with an instance profile and the instance profile must have write access to the destination. You cannot use AWS keys.\n  region - (Optional) S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, endpoint is used.\n  endpoint - (Optional) S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, endpoint is used.\n  enable_encryption - (Optional) Enable server side encryption, false by default.\n  encryption_type - (Optional) The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled and the default type is sse-s3.\n  kms_key - (Optional) KMS key used if encryption is enabled and encryption type is set to sse-kms.\n  canned_acl - (Optional) Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACL can be found here. By default only the object owner gets full control. If you are using cross account role for writing data, you may want to set bucket-owner-full-control to make bucket owner able to read the logs.\n       - init_scripts (Optional) :  Usage DBFS S3  init_scripts { dbfs { destination = \u0026#34;dbfs:/my/path/in/dbfs\u0026#34; } }   init_scripts { s3 { destination = \u0026#34;dbfs:/my/path/in/dbfs\u0026#34; region = \u0026#34;us-east-1\u0026#34; endpoint = \u0026#34;https://s3-us-east-1.amazonaws.com.\u0026#34; } }       The configuration for storing init scripts. Any number of scripts can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to \u0026lt;destination\u0026gt;/\u0026lt;cluster-id\u0026gt;/init_scripts.     dbfs - Configuration for the init scripts configuration\n destination - (Optional) DBFS location of init script. Destination must be provided. For example, \u0026ldquo;dbfs:/home/cluster_log\u0026rdquo;    s3 - Configuration for the s3 init scripts configuration\n  destination - (Optional) S3 destination, e.g. s3://my-bucket/some-prefix You must configure the cluster with an instance profile and the instance profile must have write access to the destination. You cannot use AWS keys.\n  region - (Optional) S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, endpoint is used.\n  endpoint - (Optional) S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, endpoint is used.\n  enable_encryption - (Optional) Enable server side encryption, false by default.\n  encryption_type - (Optional) The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled and the default type is sse-s3.\n  kms_key - (Optional) KMS key used if encryption is enabled and encryption type is set to sse-kms.\n  canned_acl - (Optional) Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACL can be found here. By default only the object owner gets full control. If you are using cross account role for writing data, you may want to set bucket-owner-full-control to make bucket owner able to read the logs.\n       - docker_image (Optional) :  Usage docker_image { url = \u0026#34;https://hub.docker.com/_/alpine\u0026#34; username = \u0026#34;my-user-name\u0026#34; password = \u0026#34;password\u0026#34; }    Docker image for a custom container.     url - (Required) URL for the Docker image.\n  username - (Required) User name for the Docker repository.\n  password - (Required) Password for the Docker repository. (Sensitive field)\n     - spark_env_vars:  (Optional) An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pair of the form (X,Y) are exported as is (i.e., export X='Y\u0026rsquo;) while launching the driver and workers. To specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the example below. This ensures that all default databricks managed environmental variables are included as well.\n - autotermination_minutes:  (Optional) Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.\n - enable_elastic_disk:  (Optional) Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly - refer to Autoscaling local storage for details.\n - instance_pool_id:  (Optional) The optional ID of the instance pool to which the cluster belongs. Refer to Instance Pools API for details.\n - idempotency_token:  (Optional) An optional token that can be used to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the ID of the existing cluster instead. The existence of a cluster with the same token is not checked against terminated clusters. If you specify the idempotency token, upon failure you can retry until the request succeeds. Databricks will guarantee that exactly one cluster will be launched with that idempotency token. This token should have at most 64 characters.\n Libraries Libraries are set objects within the Cluster resources, examples below.\nJar Egg Whl PyPi Maven Cran  library_jar { path = \u0026#34;dbfs:/my/path/in/dbfs/jar\u0026#34; }   library_egg { path = \u0026#34;dbfs:/my/path/in/dbfs/egg\u0026#34; }   library_whl { path = \u0026#34;dbfs:/my/path/in/dbfs/whl\u0026#34; }   library_pypi { package = \u0026#34;networkx\u0026#34; repo = \u0026#34;https://pypi.org\u0026#34; }   library_maven { coordinates = \u0026#34;org.jsoup:jsoup:1.7.2\u0026#34; repo = \u0026#34;https://mavencentral.org\u0026#34; exclusions = [\u0026#34;slf4j:slf4j\u0026#34;] }   library_cran { package = \u0026#34;ada\u0026#34; repo = \u0026#34;https://cran.us.r-project.org\u0026#34; }     - library_jar (Optional) :    URI of the JAR to be installed. DBFS and S3 URIs are supported. For example: \u0026#39;dbfs:/mnt/databricks/library.jar\u0026#39;, \u0026#39;s3://my-bucket/library.jar\u0026#39;. If S3 is used, make sure the cluster has read access on the library. You may need to launch the cluster with an instance profile to access the S3 URI.     path - (Required) Path of the jar in dbfs or in S3. For example: \u0026ldquo;dbfs:/mnt/databricks/library.jar\u0026rdquo;, \u0026ldquo;s3://my-bucket/library.jar\u0026rdquo;.\n  messages - (Required) Messages of the results of the library installation\n  status - (Computed) The status of the library installation. Possible statuses are: PENDING, RESOLVING, INSTALLING, INSTALLED, FAILED, and UNINSTALL_ON_RESTART.\n     - library_egg (Optional) :    URI of the egg to be installed. DBFS and S3 URIs are supported. For example: \u0026#39;dbfs:/my/egg\u0026#39;, \u0026#39;s3://my-bucket/egg\u0026#39; }. If S3 is used, make sure the cluster has read access on the library. You may need to launch the cluster with an instance profile to access the S3 URI.     path - (Required) Path of the egg in dbfs or in S3. For example: \u0026ldquo;dbfs:/mnt/databricks/library.egg\u0026rdquo;, \u0026ldquo;s3://my-bucket/library.egg\u0026rdquo;.\n  messages - (Required) Messages of the results of the library installation\n  status - (Computed) The status of the library installation. Possible statuses are: PENDING, RESOLVING, INSTALLING, INSTALLED, FAILED, and UNINSTALL_ON_RESTART.\n     - library_whl (Optional) :    If whl, URI of the wheel or zipped wheels to be installed. DBFS and S3 URIs are supported. For example: \u0026#39;dbfs:/my/whl\u0026#39;, \u0026#39;s3://my-bucket/whl\u0026#39;. If S3 is used, make sure the cluster has read access on the library. You may need to launch the cluster with an instance profile to access the S3 URI. Also the wheel file name needs to use the correct convention. If zipped wheels are to be installed, the file name suffix should be .wheelhouse.zip.     path - (Required) Path of the whl in dbfs or in S3. For example: \u0026ldquo;dbfs:/mnt/databricks/library.whl\u0026rdquo;, \u0026ldquo;s3://my-bucket/library.whl\u0026rdquo;.\n  messages - (Required) Messages of the results of the library installation\n  status - (Computed) The status of the library installation. Possible statuses are: PENDING, RESOLVING, INSTALLING, INSTALLED, FAILED, and UNINSTALL_ON_RESTART.\n     - library_pypi (Optional) :    Specification of a PyPI library to be installed.     package - (Required) The name of the PyPI package to install. An optional exact version specification is also supported. Examples: simplejson and simplejson==3.8.0. This field is required.\n  repo - (Optional) The repository where the package can be found. If not specified, the default pip index is used.\n  messages - (Computed) Messages of the results of the library installation\n  status - (Computed) The status of the library installation. Possible statuses are: PENDING, RESOLVING, INSTALLING, INSTALLED, FAILED, and UNINSTALL_ON_RESTART.\n     - library_maven (Optional) :    Specification of a Maven library to be installed.     coordinates - (Required) Gradle-style Maven coordinates. For example: org.jsoup:jsoup:1.7.2. This field is required.\n  repo - (Optional) Maven repo to install the Maven package from. If omitted, both Maven Central Repository and Spark Packages are searched.\n  exclusions - (Optional) List of dependences to exclude. For example: (\u0026ldquo;slf4j:slf4j\u0026rdquo;, \u0026ldquo;*:hadoop-client\u0026rdquo;).\n  messages - (Computed) Messages of the results of the library installation\n  status - (Computed) The status of the library installation. Possible statuses are: PENDING, RESOLVING, INSTALLING, INSTALLED, FAILED, and UNINSTALL_ON_RESTART.\n     - library_cran (Optional) :    Specification of a CRAN library to be installed.     package - (Required) The name of the CRAN package to install. This field is required.\n  repo - (Optional) The repository where the package can be found. If not specified, the default CRAN repo is used.\n  messages - (Computed) Messages of the results of the library installation\n  status - (Computed) The status of the library installation. Possible statuses are: PENDING, RESOLVING, INSTALLING, INSTALLED, FAILED, and UNINSTALL_ON_RESTART.\n     Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the cluster object.\n - cluster_id:  Canonical identifier for the cluster.\n - default_tags:  Tags that are added by Databricks by default, regardless of any custom_tags that may have been added. These include: Vendor: Databricks, Creator: \u0026lt;username_of_creator\u0026gt;, ClusterName: \u0026lt;name_of_cluster\u0026gt;, ClusterId: \u0026lt;id_of_cluster\u0026gt;, Name:  - state:  State of the cluster.\n - state_message:  A message associated with the most recent state transition (e.g., the reason why the cluster entered a TERMINATED state). This field is unstructured, and its exact format is subject to change.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/dbfs_file/",
	"title": "dbfs_file",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_dbfs_file This is a resource that lets you create, get and delete files in DBFS (Databricks File System).\nExample Usage resource \u0026#34;databricks_dbfs_file\u0026#34; \u0026#34;my_dbfs_file\u0026#34; { content = filebase64(\u0026#34;README.md\u0026#34;) path = \u0026#34;/sri/terraformdbfs/example/README.md\u0026#34; overwrite = true mkdirs = true validate_remote_file = true } Argument Reference The following arguments are supported:\n- content:  (Required) The content of the file as a base64 encoded string.\n - path:  (Required) The path of the file in which you wish to save.\n - overwrite:  (Optional) This is used to determine whether it should delete the existing file when with the same name when it writes. The default is set to false.\n - mkdirs:  (Optional) When the resource is created, this field is used to determine if it needs to make the parent directories. The default value is set to true.\n - validate_remote_file:  (Optional) This is used to compare the actual contents of the file to determine if the remote file is valid or not. If the base64 content is different it will attempt to do a delete, create.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the dbfs file object.\n - file_size:  The file size of the file that is being tracked by this resource in bytes.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/dbfs_file_sync/",
	"title": "dbfs_file_sync",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_dbfs_file_sync This resource will let you create a dbfs file sync which will synchronize files between systems depending on file size changes.\nThis dbfs file sync resource only determines differences via file size so if data changes but file size does not it will provide a false negative in terms of determining difference.\n Example Usage data \u0026#34;databricks_dbfs_file\u0026#34; \u0026#34;my_dbfs_file_data\u0026#34; { path = \u0026#34;/install_wheels_sri.sh\u0026#34; limit_file_size = true } resource \u0026#34;databricks_dbfs_file_sync\u0026#34; \u0026#34;my-file1-sync\u0026#34; { src_path = data.databricks_dbfs_file.my_dbfs_file_data.path tgt_path = \u0026#34;/terraformdbfs/example/wheels.sh\u0026#34; file_size = data.databricks_dbfs_file.my-dbfs-file-data.file_size mkdirs = true host = \u0026#34;https://\u0026lt;domain\u0026gt;.com/\u0026#34; token = \u0026#34;dapiherehereherehere\u0026#34; } Argument Reference The following arguments are supported:\n- src_path:  (Required) The source path in dbfs where to fetch the file from. It should look like \u0026ldquo;dbfs:/path/to/my/file\u0026rdquo;.\n - tgt_path:  (Required) The target path in dbfs where to sync the file to. It should look like \u0026ldquo;dbfs:/path/to/my/file\u0026rdquo;.\n - file_size:  (Required) This is the value that is used to determine file change. Unfortunately at this point in time there are no file checksums provided by the dbfs api. To download the file everytime terraform provides a Read operation during state refresh is not efficient.\n - mkdirs:  (Optional) This is whether the resource should create the required, parent directories to sync the file. The default value is true.\n - host:  (Optional) This is an optional api host for the databricks api if the source file resides on another workspace/dbfs system.\n - token:  (Optional) This is an optional api token for the databricks api if the source file resides on another workspace/dbfs system.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  Id for the file sync object.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/instance_pool/",
	"title": "instance_pool",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_instance_pool This resource allows you to manage instance pools on Databricks.\nAn instance pool reduces cluster start and auto-scaling times by maintaining a set of idle, ready-to-use cloud instances. When a cluster attached to a pool needs an instance, it first attempts to allocate one of the pool’s idle instances. If the pool has no idle instances, it expands by allocating a new instance from the instance provider in order to accommodate the cluster’s request. When a cluster releases an instance, it returns to the pool and is free for another cluster to use. Only clusters attached to a pool can use that pool’s idle instances.\nExample Usage .. Note:: It is important to know what that different cloud service providers have different node_type_id, disk_specs and potentially other configurations.\nInstance Pool Example AWS Azure  resource \u0026#34;databricks_instance_pool\u0026#34; \u0026#34;my-pool\u0026#34; { instance_pool_name = \u0026#34;demo-terraform-pool\u0026#34; min_idle_instances = 0 max_capacity = 5 node_type_id = \u0026#34;i3.xlarge\u0026#34; aws_attributes { availability = \u0026#34;ON_DEMAND\u0026#34; zone_id = \u0026#34;us-east-1a\u0026#34; spot_bid_price_percent = \u0026#34;100\u0026#34; } idle_instance_autotermination_minutes = 10 disk_spec = { ebs_volume_type = \u0026#34;GENERAL_PURPOSE_SSD\u0026#34; disk_size = 80 disk_count = 1 } custom_tags = { \u0026#34;creator\u0026#34;: \u0026#34;Sriharsha Tikkireddy\u0026#34; \u0026#34;testChange\u0026#34;: \u0026#34;Sri Tikkireddy\u0026#34; } }   resource \u0026#34;databricks_instance_pool\u0026#34; \u0026#34;my-pool\u0026#34; { instance_pool_name = \u0026#34;demo-terraform-pool\u0026#34; min_idle_instances = 0 max_capacity = 5 node_type_id = \u0026#34;Standard_DS3_v2\u0026#34; idle_instance_autotermination_minutes = 10 disk_spec = { azure_disk_volume_type = \u0026#34;PREMIUM_LRS\u0026#34; disk_size = 80 disk_count = 1 } custom_tags = { \u0026#34;creator\u0026#34;: \u0026#34;demo user\u0026#34; \u0026#34;testChange\u0026#34;: \u0026#34;demo user\u0026#34; } }     Argument Reference The following arguments are supported:\n- instance_pool_name:  (Required) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n - min_idle_instances:  (Required) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n - max_capacity:  (Required) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling.\n - idle_instance_autotermination_minutes:  (Required) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n - aws_attributes (Optional) :    Attributes related to instance pools running on Amazon Web Services. If not specified at creation time, a set of default values is used. This block contains the following attributes of availability, zone_id, spot_bid_price_percent:    availability - (Optional) Availability type used for all instances in the pool. Only \u0026quot;ON_DEMAND\u0026quot; and \u0026quot;SPOT\u0026quot; are supported. zone_id - (Optional) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of a form like \u0026quot;us-west-2a\u0026quot;. The provided availability zone must be in the same region as the Databricks deployment. For example, \u0026quot;us-west-2a\u0026quot; is not a valid zone ID if the Databricks deployment resides in the \u0026quot;us-east-1\u0026quot; region. This is an optional field. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the List Zones API. spot_bid_price_percent - (Optional) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. For safety, this field cannot be greater than 10000.  .. Important:: aws_attributes will only work for instance pools in an AWS deployment of Databricks. They will not work on Azure Databricks!\n   - node_type_id:  (Required) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the List Node Types API call.\n - custom_tags:  (Optional) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026amp; Azure instances and Disk volumes) with these tags in addition to default_tags. Databricks allows at most 43 custom tags.\n - enable_elastic_disk:  (Optional) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n - disk_spec (Optional) :   For disk_spec make sure to use ebs_volume_type only on AWS deployment of Databricks and azure_disk_volume_type only on a Azure deployment of Databricks.\n   Defines the amount of initial remote storage attached to each instance in the pool. This block contains the following attributes of ebs_volume_type, azure_disk_volume_type, disk_count, disk_size:     ebs_volume_type - (Optional) The EBS volume type to use. Options are: \u0026quot;GENERAL_PURPOSE_SSD\u0026quot; (Provision extra storage using AWS gp2 EBS volumes) or \u0026quot;THROUGHPUT_OPTIMIZED_HDD\u0026quot; (Provision extra storage using AWS st1 volumes.)\n  azure_disk_volume_type - (Optional) The type of Azure disk to use. Options are: \u0026quot;PREMIUM_LRS\u0026quot; (Premium storage tier, backed by SSDs) or \u0026quot;STANDARD_LRS\u0026quot; (Standard storage tier, backed by HDDs.)\n  disk_count - (Optional) The number of disks to attach to each instance:\n This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.    disk_size - (Optional) The size of each disk (in GiB) to attach. Values must fall into the supported range for a particular instance type:\n AWS (ebs):  General Purpose SSD: 100 - 4096 GiB Throughput Optimized HDD: 500 - 4096 GiB   Azure (disk volume):  Premium LRS (SSD): 1 - 1023 GiB Standard LRS (HDD): 1- 1023 GiB       \n - preloaded_spark_versions:  (Optional) A list with the runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do have to wait for the image to download. You can retrieve a list of available runtime versions by using the Runtime Versions API call.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  :ref:id \u0026lt;r_instance_pool_id\u0026gt; - The id for the instance pool object.  - default_tags:  Tags that are added by Databricks regardless of any custom_tags, including:\n Vendor: Databricks DatabricksInstancePoolCreatorId: \u0026lt;create_user_id\u0026gt; DatabricksInstancePoolId: \u0026lt;instance_pool_id\u0026gt;   - state:  Current state of the instance pool.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/instance_profile/",
	"title": "instance_profile",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_instance_profile This resource allows you to create, get, and delete instance profiles that users can launch clusters with.\nExample Usage resource \u0026#34;databricks_instance_profile\u0026#34; \u0026#34;db-instance-profile\u0026#34; { instance_profile_arn = \u0026#34;arn:aws:iam::999999999999:instance-profile/custom-s3-access-instance-profile\u0026#34; skip_validation = true } Argument Reference The following arguments are supported:\n- instance_profile_arn:  (Required)\n - skip_validation:  (Required)\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the instance profile object.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/job/",
	"title": "job",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_job The databricks_job resource allows you to create, edit, and delete jobs.\nExample Usage Databricks Example New Automated Cluster Existing Cluster  resource \u0026#34;databricks_job\u0026#34; \u0026#34;my_job3\u0026#34; { new_cluster { autoscale { min_workers = 2 max_workers = 3 } spark_version = \u0026#34;6.4.x-scala2.11\u0026#34; aws_attributes { zone_id = data.databricks_zones.my_wspace_zones.default_zone spot_bid_price_percent = \u0026#34;100\u0026#34; } node_type_id = \u0026#34;r3.xlarge\u0026#34; } notebook_path = \u0026#34;/Users/jane.doe@databricks.com/my-demo-notebook\u0026#34; name = \u0026#34;my-demo-notebook\u0026#34; timeout_seconds = 3600 max_retries = 1 max_concurrent_runs = 1 }   resource \u0026#34;databricks_job\u0026#34; \u0026#34;my_job3\u0026#34; { existing_cluster_id = \u0026#34;\u0026lt;Cluster ID\u0026gt;\u0026#34; notebook_path = \u0026#34;/Users/jane.doe@databricks.com/my-demo-notebook\u0026#34; name = \u0026#34;my-demo-notebook\u0026#34; timeout_seconds = 3600 max_retries = 1 max_concurrent_runs = 1 }     Argument Reference The following arguments are supported:\n- existing_cluster_id:  (Optional) If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We suggest running jobs on new clusters for greater reliability.\n - new_cluster:  (Optional) A description of a cluster that will be created for each run. Please look below.\n - name:  (Optional) An optional name for the job. The default value is Untitled.\n - library_jar:  (Optional) URI of the JAR to be installed. DBFS and S3 URIs are supported. For example: \u0026ldquo;dbfs:/mnt/databricks/library.jar\u0026rdquo;, \u0026ldquo;s3://my-bucket/library.jar\u0026rdquo;. If S3 is used, make sure the cluster has read access on the library. You may need to launch the cluster with an instance profile to access the S3 URI.\n - library_egg:  (Optional) URI of the egg to be installed. DBFS and S3 URIs are supported. For example: \u0026ldquo;dbfs:/my/egg\u0026rdquo;, \u0026ldquo;s3://my-bucket/egg\u0026rdquo; }. If S3 is used, make sure the cluster has read access on the library. You may need to launch the cluster with an instance profile to access the S3 URI.\n - library_whl:  (Optional) If whl, URI of the wheel or zipped wheels to be installed. DBFS and S3 URIs are supported. For example: \u0026ldquo;dbfs:/my/whl\u0026rdquo;, \u0026ldquo;s3://my-bucket/whl\u0026rdquo;. If S3 is used, make sure the cluster has read access on the library. You may need to launch the cluster with an instance profile to access the S3 URI. Also the wheel file name needs to use the correct convention. If zipped wheels are to be installed, the file name suffix should be .wheelhouse.zip.\n - library_pypi (Optional) :    Specification of a PyPI library to be installed.     package - (Required) The name of the PyPI package to install. An optional exact version specification is also supported. Examples: simplejson and simplejson==3.8.0. This field is required.\n  repo - (Optional) The repository where the package can be found. If not specified, the default pip index is used.\n     - library_maven (Optional) :    Specification of a Maven library to be installed.     coordinates - (Required) Gradle-style Maven coordinates. For example: org.jsoup:jsoup:1.7.2. This field is required.\n  repo - (Optional) Maven repo to install the Maven package from. If omitted, both Maven Central Repository and Spark Packages are searched.\n  exclusions - (Optional) List of dependences to exclude. For example: (\u0026ldquo;slf4j:slf4j\u0026rdquo;, \u0026ldquo;*:hadoop-client\u0026rdquo;).\n     - library_cran (Optional) :    Specification of a CRAN library to be installed.     package - (Required) The name of the CRAN package to install. This field is required.\n  repo - (Optional) The repository where the package can be found. If not specified, the default CRAN repo is used.\n     - notebook_path:  (Optional) The absolute path of the notebook to be run in the Databricks Workspace. This path must begin with a slash. This field is required.\n - notebook_base_parameters:  (Optional) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used.\n - jar_uri:  (Optional) Deprecated since 04/2016. Provide a jar through the libraries field instead.\n - jar_main_class_name:  (Optional) The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use SparkContext.getOrCreate to obtain a Spark context; otherwise, runs of the job will fail.\n - jar_parameters:  (Optional) Parameters that will be passed to the main method.\n - python_file:  (Optional) The URI of the Python file to be executed. DBFS and S3 paths are supported. This field is required.\n - python_parameters:  (Optional) Command line parameters that will be passed to the Python file.\n - spark_submit_parameters:  (Optional) Command-line parameters passed to spark submit.\n - email_notifications (Optional) :    An optional set of email addresses notified when runs of this job begin and complete and when this job is deleted. The default behavior is to not send any emails.     on_start - (Optional) A list of email addresses to be notified when a run begins. If not specified upon job creation or reset, the list will be empty, i.e., no address will be notified.\n  :on_success - (Optional) A list of email addresses to be notified when a run successfully completes. A run is considered to have completed successfully if it ends with a TERMINATED life_cycle_state and a SUCCESSFUL result_state. If not specified upon job creation or reset, the list will be empty, i.e., no address will be notified.\n  on_failure - (Optional) A list of email addresses to be notified when a run unsuccessfully completes. A run is considered to have completed unsuccessfully if it ends with an INTERNAL_ERROR life_cycle_state or a SKIPPED, FAILED, or TIMED_OUT result_state. If not specified upon job creation or reset, the list will be empty, i.e., no address will be notified.\n  no_alert_for_skipped_runs - (Optional) If true, do not send email to recipients specified in on_failure if the run is skipped.\n     - timeout_seconds:  (Optional) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n - max_retries:  (Optional) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED result_state or INTERNAL_ERROR life_cycle_state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.\n - min_retry_interval_millis:  (Optional) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n - retry_on_timeout:  (Optional) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n - job_schedule (Optional) :    An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow.     quartz_cron_expression - (Optional) A Cron expression using Quartz syntax that describes the schedule for a job. See Cron Trigger for details. This field is required.\n  timezone_id - (Optional) A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.\n     - max_concurrent_runs:  (Optional) An optional maximum allowed number of concurrent runs of the job. Set this value if you want to be able to execute multiple runs of the same job concurrently. This is useful for example if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or if you want to trigger multiple runs which differ by their input parameters. This setting affects only new runs. For example, suppose the job’s concurrency is 4 and there are 4 concurrent active runs. Then setting the concurrency to 3 won’t kill any of the active runs. However, from then on, new runs are skipped unless there are fewer than 3 active runs. This value cannot exceed 150. Setting this value to 0 causes all new runs to be skipped. The default behavior is to allow only 1 concurrent run.\n New Cluster - num_workers (Optional) :  (Optional) Number of worker nodes that this cluster should have. A cluster has one Spark Driver and num_workers Executors for a total of num_workers + 1 Spark node\n - autoscale (Optional) :    Parameters needed in order to automatically scale clusters up and down based on load.   Usage resource \u0026#34;databricks_cluster\u0026#34; \u0026#34;my-cluster\u0026#34; { ... autoscale { min_workers = 2 max_workers = 3 } ... }   min_workers - (Optional) The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.\n  max_workers - (Optional) The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.\n     - cluster_name:  (Optional) Cluster name. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n - spark_version:  (Optional) The Spark version of the cluster. A list of available Spark versions can be retrieved by using the Runtime Versions API call. This field is required.\n - spark_conf:  (Optional) An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively.\n - aws_attributes (Optional) :    Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used.   Usage resource \u0026#34;databricks_cluster\u0026#34; \u0026#34;my-cluster\u0026#34; { ... aws_attributes { zone_id = \u0026#34;us-east-1\u0026#34; availability = \u0026#34;SPOT\u0026#34; spot_bid_price_percent = 100 instance_profile_arn = \u0026#34;arn:aws:iam::999999999999:instance-profile/custom-s3-access-instance-profile\u0026#34; first_on_demand = 1 ebs_volume_type = \u0026#34;GENERAL_PURPOSE_SSD\u0026#34; ebs_volume_count = 1 ebs_volume_size = 32 } ... }   zone_id - (Required) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like “us-west-2a”. The provided availability zone must be in the same region as the Databricks deployment. For example, “us-west-2a” is not a valid zone ID if the Databricks deployment resides in the “us-east-1” region.\n  availability - (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Note: If first_on_demand is zero, this availability type will be used for the entire cluster.\n  spot_bid_price_percent - (Optional) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000.\n  instance_profile_arn - (Optional) Nodes for this cluster will only be placed on AWS instances with this instance profile. If omitted, nodes will be placed on instances without an instance profile. The instance profile must have previously been added to the Databricks environment by an account administrator.\n  first_on_demand - (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.\n  ebs_volume_type - (Optional) The type of EBS volumes that will be launched with this cluster. GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD\n  ebs_volume_count - (Optional) The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.\n  ebs_volume_size - (Optional) The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).\n     - driver_node_type_id:  (Optional) The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above.\n - node_type_id:  (Optional) This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List Node Types API call. This field is required.\n - ssh_public_keys:  (Optional) SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. Up to 10 keys can be specified.\n - custom_tags:  (Optional) Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS instances and EBS volumes) with these tags in addition to default_tags.\n - cluster_log_conf (Optional) :  Usage DBFS S3  cluster_log_conf { dbfs { destination = \u0026#34;dbfs:/my/path/in/dbfs\u0026#34; } }   cluster_log_conf { s3 { destination = \u0026#34;s3:/my/path/in/dbfs\u0026#34; region = \u0026#34;us-east-1\u0026#34; endpoint = \u0026#34;https://s3-us-east-1.amazonaws.com\u0026#34; enable_encryption = true encryption_type = \u0026#34;sse-kms\u0026#34; kms_key = \u0026#34;my-kms-key-here\u0026#34; canned_acl = \u0026#34;bucket-owner-full-control\u0026#34; } }       The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is \u0026lt;destination\u0026gt;/\u0026lt;cluster-id\u0026gt;/driver, while the destination of executor logs is \u0026lt;destination\u0026gt;/\u0026lt;cluster-id\u0026gt;/executor.     dbfs - Configuration for the dbfs cluster logs configuration\n destination - (Optional) DBFS location of cluster log. destination must be provided. For example, \u0026ldquo;dbfs:/home/cluster_log\u0026rdquo;    s3 - Configuration for the s3 cluster logs configuration\n  destination - (Optional) S3 destination, e.g. s3://my-bucket/some-prefix You must configure the cluster with an instance profile and the instance profile must have write access to the destination. You cannot use AWS keys.\n  region - (Optional) S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, endpoint is used.\n  endpoint - (Optional) S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, endpoint is used.\n  enable_encryption - (Optional) Enable server side encryption, false by default.\n  encryption_type - (Optional) The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled and the default type is sse-s3.\n  kms_key - (Optional) KMS key used if encryption is enabled and encryption type is set to sse-kms.\n  canned_acl - (Optional) Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACL can be found here. By default only the object owner gets full control. If you are using cross account role for writing data, you may want to set bucket-owner-full-control to make bucket owner able to read the logs.\n       - init_scripts (Optional) :  Usage DBFS S3  init_scripts { dbfs { destination = \u0026#34;dbfs:/my/path/in/dbfs\u0026#34; } }   init_scripts { s3 { destination = \u0026#34;dbfs:/my/path/in/dbfs\u0026#34; region = \u0026#34;us-east-1\u0026#34; endpoint = \u0026#34;https://s3-us-east-1.amazonaws.com.\u0026#34; } }       The configuration for storing init scripts. Any number of scripts can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to \u0026lt;destination\u0026gt;/\u0026lt;cluster-id\u0026gt;/init_scripts.     dbfs - Configuration for the init scripts configuration\n destination - (Optional) DBFS location of init script. Destination must be provided. For example, \u0026ldquo;dbfs:/home/cluster_log\u0026rdquo;    s3 - Configuration for the s3 init scripts configuration\n  destination - (Optional) S3 destination, e.g. s3://my-bucket/some-prefix You must configure the cluster with an instance profile and the instance profile must have write access to the destination. You cannot use AWS keys.\n  region - (Optional) S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, endpoint is used.\n  endpoint - (Optional) S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, endpoint is used.\n  enable_encryption - (Optional) Enable server side encryption, false by default.\n  encryption_type - (Optional) The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled and the default type is sse-s3.\n  kms_key - (Optional) KMS key used if encryption is enabled and encryption type is set to sse-kms.\n  canned_acl - (Optional) Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACL can be found here. By default only the object owner gets full control. If you are using cross account role for writing data, you may want to set bucket-owner-full-control to make bucket owner able to read the logs.\n       - spark_env_vars:  (Optional) An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pair of the form (X,Y) are exported as is (i.e., export X='Y\u0026rsquo;) while launching the driver and workers. To specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the example below. This ensures that all default databricks managed environmental variables are included as well.\n - enable_elastic_disk:  (Optional) Autoscaling Local Storage: when enabled, this cluster dynamically acquires additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly - refer to Autoscaling local storage for details.\n - instance_pool_id:  (Optional) The optional ID of the instance pool to which the cluster belongs. Refer to Instance Pools API for details.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the job object.\n - job_id:  The canonical identifier for the newly created job.\n - creator_user_name:  The creator user name. This field won’t be included in the response if the user has already been deleted.\n - created_time:  The time at which this job was created in epoch milliseconds (milliseconds since 1/1/1970 UTC).\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/notebook/",
	"title": "notebook",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_notebook This resource allows you to manage the import, export, and delete notebooks. The maximum allowed size of a request to resource is 10MB.\nExample Usage resource \u0026#34;databricks_notebook\u0026#34; \u0026#34;my_databricks_notebook\u0026#34; { content = filebase64(\u0026#34;${path.module}/demo-terraform.dbc\u0026#34;) path = \u0026#34;/workspace/terraform-test-folder/\u0026#34; overwrite = false mkdirs = true format = \u0026#34;DBC\u0026#34; } Argument Reference The following arguments are supported:\n- content:  (Required) The base64-encoded content. If the limit (10MB) is exceeded, exception with error code MAX_NOTEBOOK_SIZE_EXCEEDED will be thrown.\n - path:  (Required) The absolute path of the notebook or directory. Exporting a directory is supported only for DBC. This field is required.\n - language:  (Required) The language. If format is set to SOURCE, this field is required; otherwise, it will be ignored. Possible choices are SCALA, PYTHON, SQL, R.\n - overwrite:  (Required) The flag that specifies whether to overwrite existing object. It is false by default. For DBC format, overwrite is not supported since it may contain a directory.\n - mkdirs:  (Required) Create the given directory and necessary parent directories if they do not exists. If there exists an object (not a directory) at any prefix of the input path, this call returns an error RESOURCE_ALREADY_EXISTS. If this operation fails it may have succeeded in creating some of the necessary parent directories.\n - format:  (Required) This specifies the format of the file to be imported. By default, this is SOURCE. However it may be one of: SOURCE, HTML, JUPYTER, DBC. The value is case sensitive.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the notebook object.\n - object_id:  Unique identifier for a NOTEBOOK or DIRECTORY.\n - object_type:  The type of the object. It could be NOTEBOOK, DIRECTORY or LIBRARY.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": "Databricks Provider Resources "
},
{
	"uri": "/resources/scim_group/",
	"title": "scim_group",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_scim_group This resource allows you to create groups in Databricks. You can also associate Databricks users to the following groups.\nYou must be a Databricks administrator API token to use SCIM resources.\n Example Usage resource \u0026#34;databricks_scim_user\u0026#34; \u0026#34;my-user\u0026#34; { user_name = \u0026#34;testuser@databricks.com\u0026#34; display_name = \u0026#34;Test User\u0026#34; entitlements = [ \u0026#34;allow-cluster-create\u0026#34;, ] } resource \u0026#34;databricks_scim_group\u0026#34; \u0026#34;my-group\u0026#34; { display_name = \u0026#34;Sri Test Group\u0026#34; members = [\u0026#34;${databricks_scim_user.my-user.id}\u0026#34;] } Argument Reference The following arguments are supported:\n- display_name:  (Required) This is the display name for the given group.\n - group_members:  (Optional) This is a list of users associated to the given group.\n - roles:  (Optional) This is the list of roles that you wish to attach to this group.\n - entitlements:  (Optional) This is the list of entitlements that you wish to attach to this group.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the scim group object.\n - inherited_roles:  The list of roles inherited by parent groups that this group is a member of.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/scim_user/",
	"title": "scim_user",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_scim_user This resource allows you to create users in Databricks and give them the proper level of access, as well as remove access for users (deprovision them) when they leave your organization or no longer need access to Databricks.\nYou must be a Databricks administrator API token to use SCIM resources.\n This resource is heavily reliant on inherited group information and the default_roles object, to determine deltas. What this means is that, even if you change the roles field, if it is inherited it will ignore the change as it is inherited by parent group. It will only detect delta when it is a net new role or a net new delete not covered by inherited roles or default roles.\n Example Usage resource \u0026#34;databricks_scim_user\u0026#34; \u0026#34;my-user\u0026#34; { user_name = \u0026#34;testuser@databricks.com\u0026#34; display_name = \u0026#34;Test User\u0026#34; entitlements = [ \u0026#34;allow-cluster-create\u0026#34;, ] } Argument Reference The following arguments are supported:\n- user_name:  (Required) This is the username of the given user and will be their form of access and identity.\n - display_name:  (Optional) This is an alias for the username can be the full name of the user.\n - roles:  (Optional) This is a list of roles assigned to the user, specific to the AWS environment for user to assume roles on clusters.\n - entitlements:  (Optional) Entitlements for the user to be able to have the ability to create clusters and pools. Current options are: \u0026quot;allow-cluster-create\u0026quot;, \u0026quot;allow-instance-pool-create\u0026quot;.\n - default_roles:  (Required) Set of roles that are assigned to the all_users group in Databricks. You can use the default_user_roles data source to fetch the values for this.\n - set_admin:  (Required) Setting this to true will patch this user to include the admin group id as a group item and if false, it will patch remove this user from the admin group.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the scim user object.\n - inherited_roles:  The list of roles inherited by parent and all_users groups. This is used to determine when there are no changes.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/secret/",
	"title": "secret",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_secret With this resource you can insert a secret under the provided scope with the given name. If a secret already exists with the same name, this command overwrites the existing secret’s value. The server encrypts the secret using the secret scope’s encryption settings before storing it. You must have WRITE or MANAGE permission on the secret scope.\nThe secret key must consist of alphanumeric characters, dashes, underscores, and periods, and cannot exceed 128 characters. The maximum allowed secret value size is 128 KB. The maximum number of secrets in a given scope is 1000.\nYou can read a secret value only from within a command on a cluster (for example, through a notebook); there is no API to read a secret value outside of a cluster. The permission applied is based on who is invoking the command and you must have at least READ permission.\nExample Usage resource \u0026#34;databricks_secret_scope\u0026#34; \u0026#34;my-scope\u0026#34; { name = \u0026#34;terraform-demo-scope\u0026#34; initial_manage_principal = \u0026#34;users\u0026#34; } resource \u0026#34;databricks_secret\u0026#34; \u0026#34;my_secret\u0026#34; { key = \u0026#34;test-secret-1\u0026#34; string_value = \u0026#34;hello world 123\u0026#34; scope = \u0026#34;${databricks_secret_scope.my-scope.name}\u0026#34; } Argument Reference The following arguments are supported:\n- string_value:  (Required) If string_value, if specified, the value will be stored in UTF-8 (MB4) form.\n - scope:  (Required) The name of the scope to which the secret will be associated with. This field is required.\n - key:  (Required) A unique name to identify the secret. This field is required.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the secret object.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/secret_acl/",
	"title": "secret_acl",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_secret_acl Create or overwrite the ACL associated with the given principal (user or group) on the specified scope point. In general, a user or group will use the most powerful permission available to them, and permissions are ordered as follows:\n MANAGE - Allowed to change ACLs, and read and write to this secret scope. WRITE - Allowed to read and write to this secret scope. READ - Allowed to read this secret scope and list what secrets are available.  Example Usage resource \u0026#34;databricks_secret_scope\u0026#34; \u0026#34;my-scope\u0026#34; { name = \u0026#34;terraform-demo-scope\u0026#34; } resource \u0026#34;databricks_secret_acl\u0026#34; \u0026#34;my-acl\u0026#34; { principal = \u0026#34;USERS\u0026#34; permission = \u0026#34;READ\u0026#34; scope = \u0026#34;${databricks_secret_scope.my-scope.name}\u0026#34; } Argument Reference The following arguments are supported:\n- scope:  (Required) The name of the scope to remove permissions from. This field is required. (MB4) form.\n - principal:  (Required) The principal to remove an existing ACL from. The principal is a user or group name corresponding to an existing Databricks principal to be granted or revoked access. This field is required.\n - permission:  (Required) The permission level applied to the principal. Options are: \u0026quot;READ\u0026quot;, \u0026quot;WRITE\u0026quot;, \u0026quot;MANAGE\u0026quot;. This field is required.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the secret scope acl object.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/secret_scope/",
	"title": "secret_scope",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_secret_scope This resource creates a Databricks-backed secret scope in which secrets are stored in Databricks-managed storage and encrypted with a cloud-based specific encryption key.\nThe scope name:\n Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.  Example Usage resource \u0026#34;databricks_secret_scope\u0026#34; \u0026#34;my-scope\u0026#34; { name = \u0026#34;terraform-demo-scope\u0026#34; initial_manage_principal = \u0026#34;users\u0026#34; } Argument Reference The following arguments are supported:\n- name:  (Required) Scope name requested by the user. Scope names are unique. This field is required.\n - initial_manage_principal:  (Optional) The principal that is initially granted MANAGE permission to the created scope.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the secret scope object.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/resources/token/",
	"title": "token",
	"tags": [],
	"description": "",
	"content": "Resource: databricks_token This resource creates an api token that can be used to create Databricks resources.\nThis will create an API token for the user that has authenticated on the provider. So if you have used an admin user to setup the provider then you will be making API tokens for that admin user.\n Example Usage resource \u0026#34;databricks_token\u0026#34; \u0026#34;my-token\u0026#34; { lifetime_seconds = 6000 comment = \u0026#34;Testing terraform v2\u0026#34; } Argument Reference The following arguments are supported:\n- lifetime_seconds:  (Optional) (Numeric) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n - comment:  (Optional) Optional description to attach to the token.\n Attribute Reference In addition to all arguments above, the following attributes are exported:\n- id:  The id for the token object.\n - creation_time:  Server time (in epoch milliseconds) when the token was created.\n - token_value:  Sensitive The value of the newly-created token.\n - expiry_time:  Server time (in epoch milliseconds) when the token will expire, or -1 if not applicable.\n Import Importing this resource is not currently supported.\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]